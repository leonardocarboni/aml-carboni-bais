{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Machine Learning - programming assignment 1\n",
    "\n",
    "*Due: Friday December 2nd*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Carboni Leonardo (0279048)\n",
    "* Bais Giacomo (5355583)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further instructions:\n",
    "* Make sure your code is properly commented.\n",
    "* Submit your code in Blackboard using **one** of your accounts; we will put the grade in Blackboard for the other team member as well.\n",
    "* **Make sure to name the submitted file according to your and your collaborators last name.** (submitter_collaborator.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-armed Bandits\n",
    "\n",
    "In this programming assignment, we will look at how we can solve a k-armed bandit problem as discussed in the lecture. Expect for winning at the slot machines, you are expect to better understand the tradeoff between exploration and exploiation. \n",
    "\n",
    "Here are the objectives of this assignment:\n",
    "1.   Get familier with the Open-AI gym environment,\n",
    "2.   Implement your own k-armed bandit environment based on the gym framework,\n",
    "3.   Use an epsilon-greedy algorithm to find the optimal action for this k-armed bandit problem,\n",
    "4.   Play with the parameter epsilon and identify a reasonable setting for balancing exploration and exploiation. \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "soq1skOjrr6z"
   },
   "source": [
    "### 1. Let's start with the OpenAI gym\n",
    "\n",
    "Gym (https://gym.openai.com/) is a wide-used toolkit for developing and comparing reinforcement learning algorithms. \n",
    "\n",
    "1. Gym makes no assumptions about the structure of your agent, and is compatible with any numerical computation library, such as TensorFlow or Theano. \n",
    "\n",
    "2. The gym library is a collection of test problems — **environments** — that you can use to work out your reinforcement learning algorithms. These environments have a shared interface, allowing you to write general algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VBwBIZPkZf_g"
   },
   "source": [
    "First, we download & install the gym library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "on5JDYmWaK-w"
   },
   "source": [
    "**Great!** Now let's import the gym class and work on a basic example of gym code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5hK-jC9ceDuY"
   },
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WLePIoe7VJXS"
   },
   "source": [
    "Like mentioned above, gym's main purpose is to provide a large collection of **environments** that expose a common interface. You can find a listing of those environments below (they are Markov decision process(MDP) enviroments and we will discuss MDP in our lecture), as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import envs\n",
    "print(envs.registry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to explain how the RL framework of gym works. \n",
    "- An **ENVIRONMENT**, \n",
    "- You also have an **AGENT**,\n",
    "- The agent takes an **ACTION**, in our case, 10 actions are possible to take,\n",
    "- When a single **ACTION** is chosen and fed to our **ENVIRONMENT**, the **ENVIRONMENT** measures how good the action was taken and produces a **REWARD**, which is usually a numeric value.\n",
    "\n",
    "In MDP problems, the **ENVIRONMENT** will also provides an **OBSERVATION**, which represets the state of the **ENVIRONMENT** at the current moment. In the multi-armed bandit problems, there is no **OBSERVATION** (or state). You may understand this better after the lecture about Markov decision process (MDP). \n",
    "\n",
    "Please read the 'Getting Started with gym' https://gym.openai.com/docs/ for better understanding the framework. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QA-998XefX85"
   },
   "source": [
    "### 2. Implement your own environment\n",
    "\n",
    "Next, we are going to implement our own multi-arm bandit environment following the framework of gym. This enviroment is a gambiling room with ten different slot machines (a 10-armed bandit problem). Similar with examples given in the lecture, the reward of each slot machine follows a normal distribution, but the average reward (mean) and variance of each action are different. Your goal is to determine the optimal action from all possible actions/machines. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The core gym interface is **Env**, which is the unified environment interface. There is no interface for agents. The following are the Env methods you should know:\n",
    "\n",
    "- `step(self, action)`: Steps the environment by one timestep. Returns observation, reward, done, info.\n",
    "- `reset(self)`: Resets the environment to an initial state. Returns an initial observation. Each call of `reset()` should yield an environment suitable for a new episode, independent of previous episodes. Because there is no state transition in multi-armed bandit problems, this function is not used here.\n",
    "- `render(self, mode='human')`: Renders one frame of the environment. The default mode will do something human friendly, such as pop up a window. In this assignment, there is no need to create a pop up window. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before writing your own codes, read through the readme of github page of gym https://github.com/openai/gym. You are also recommended to read at least the codes for one simple environment and one example agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Self-defined Slot Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please fill in the missing codes in the function sample (1 point).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sGDaa_u8fjO3"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SlotMachine:\n",
    "    \"\"\"\n",
    "        A slot machine contains a reward distribution that randomly generated with restricted mean and standard deviation. \n",
    "            sample function: generates a reward at each time step based on the given reward distribition\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.mu = np.random.uniform(-5, 5)  # mean\n",
    "        self.sigma = np.random.uniform(0.5, 1)  # standard deviation\n",
    "\n",
    "    def sample(self):\n",
    "        ########## TODO: to be filled. ########## \n",
    "        return np.random.normal(loc = self.mu, scale = self.sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Game Environment\n",
    "**Please fill in the missing codes in function step (1 point) in the environment.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import spaces\n",
    "\n",
    "# The environment has to inherit the interface of gym.Env\n",
    "\n",
    "\n",
    "class GamblingRoom(gym.Env):\n",
    "    \"\"\"\n",
    "    A k-armed bandit environment: a gambling room with slot machines, allows the agents to interact with it.\n",
    "        r_machines: A list of slot machines, each GamblingRoom contains k number of SlotMachines\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, k):\n",
    "        # initialize reward distribution for each action/machine\n",
    "        self.r_machines = []\n",
    "        for i in range(k):\n",
    "            # each gamblingRoom contains k number of SlotMachines\n",
    "            self.r_machines.append(SlotMachine())\n",
    "\n",
    "        self.num_arms = k\n",
    "        self.action_space = spaces.Discrete(self.num_arms)\n",
    "        self.observation_space = spaces.Discrete(1)\n",
    "        # for our bandit environment, the state is constant\n",
    "        self.state = 0\n",
    "        self.seed()\n",
    "\n",
    "    # step up the environment based on the selected action,\n",
    "    # return the constant state, reward, done = false, and info\n",
    "    # for now, we do not have to worry about the DONE and INFO variables.\n",
    "    def step(self, action):\n",
    "        assert self.action_space.contains(action)\n",
    "        done = False\n",
    "        info = {}\n",
    "\n",
    "        ########## TODO: to be filled. ##########\n",
    "        reward = self.r_machines[action].sample()\n",
    "        return self.state, reward, done, info\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        pass\n",
    "\n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "    def render(self, mode='human', close=False):\n",
    "        for sm in self.r_machines:\n",
    "            print((sm.mu, sm.sigma))\n",
    "\n",
    "    def close(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9QFSX4fjxrh8"
   },
   "source": [
    "### 3. Implement an agent with the epsilon greedy algorithm\n",
    "\n",
    "In this part, you are expected to implement an RL agent. To decide the action to take at each time step, this agent uses the epsilon greedy algorithm introduced in the lecture.\n",
    "\n",
    "**Please fill in the missing codes in function select_action (1.5 points) and update_parameters (1 point) in the agent.** Feel free to import the needed packages if there are any."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NWI9R9BiybZl"
   },
   "outputs": [],
   "source": [
    "class EplisonGreedyAgent:\n",
    "    def __init__(self, k, e):\n",
    "        # set up the number of arms/actions\n",
    "        self.num_arms = k\n",
    "        # set up the value of epsilon\n",
    "        self.epsilon = e\n",
    "        # init the estimated values of all actions\n",
    "        self.Qvalues = np.zeros(k)\n",
    "        # init the numbers of time step that every action is selected\n",
    "        self.stepSize = np.zeros(k)\n",
    "\n",
    "    ##\n",
    "    # select the action to take at the current time step\n",
    "    # (for MDP, choose the action based on state; for k-armed bandit, no state given)\n",
    "    # return: the action to take\n",
    "    ##\n",
    "    def select_action(self):\n",
    "        ########## TODO: to be filled. ##########\n",
    "        if np.random.uniform() < self.epsilon: # epsilon chance to randomly explore\n",
    "            new_action = np.random.randint(0, self.num_arms)\n",
    "            return new_action\n",
    "        new_action = np.random.choice(np.where(self.Qvalues == self.Qvalues.max())[0])\n",
    "        return new_action\n",
    "\n",
    "    ##\n",
    "    # Update the Q-values of the agent based on received rewards\n",
    "    # input: action_index = the action, reward = the reward from this action\n",
    "    # return: null\n",
    "    ##\n",
    "    def update_parameters(self, action, reward):\n",
    "        ########## TODO: to be filled. ##########  \n",
    "        self.stepSize[action] += 1\n",
    "        self.Qvalues[action] = self.Qvalues[action] + (reward - self.Qvalues[action])/(self.stepSize[action])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "me9kN1MPWEEf"
   },
   "source": [
    "### 4. Run the simulation, play with parameters and analyse results\n",
    "\n",
    "Finally, we write codes for running the simulation. \n",
    "\n",
    "In order to decrease the effect of randomness, we usually conduct multiple simulation runs and average the results. In the implementation, you may start with one run, then use the variable `num_runs` for running multiple simulations.\n",
    "\n",
    "In each run, you shall setup the `epsilon` and number of time step `num_episodes` (0.01 and 500 by default). Then, after the initlization of our agent and environment, **please fill in the missing codes (with ??? or TODO: to be filled). (2.5 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_action = 10\n",
    "num_seed = 5\n",
    "num_runs = 100  # number of simulation runs\n",
    "num_episodes = 500  # number of steps in each run\n",
    "epsilon = 0.01\n",
    "\n",
    "# set up the random seed\n",
    "np.random.seed(num_seed)\n",
    "\n",
    "# init the environment\n",
    "env = GamblingRoom(num_action)\n",
    "\n",
    "# delete the wrap\n",
    "env = env.unwrapped\n",
    "\n",
    "# show the action space\n",
    "sims = np.zeros(num_runs)\n",
    "# run multiple simulations\n",
    "for i_run in range(num_runs):\n",
    "    ########## TODO: to be filled. ########## \n",
    "\n",
    "    # init the epsilon-greedy RL agent \n",
    "    agent = EplisonGreedyAgent(num_action, epsilon)\n",
    "    # in each simulation run, loop the action selection\n",
    "    # save the result variables you need\n",
    "    reward_tot = 0\n",
    "    for _ in range(num_episodes):\n",
    "        action = agent.select_action()\n",
    "        reward = env.step(action)[1]\n",
    "        agent.update_parameters(action, reward)\n",
    "        reward_tot += reward\n",
    "    sims[i_run] = reward_tot\n",
    "env.close()\n",
    "\n",
    "print(np.mean(sims))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MgJlSiCGbMBP"
   },
   "source": [
    "Now it's time to examine the performance of algorithms with different epsilon values (different exploration strategies) in multiple simulation runs. \n",
    "\n",
    "You shall play with the parameter epsilon under 2 or 3 different gambling environments (by initlizing different reward distributions for machines). **For each environment, try at least 2 different values of epsilon and identify a reasonable epsilon value that could balance the exploration and exploiation**. Instead of handing in your codes for this part, please select one environment you have tested and describe your environment and experimental settings **(1 point)**. Then, provide an explanation on how you identify the good epsilon value in this environment and why it is a good one **(1 point)**. \n",
    "\n",
    "Few instructions:\n",
    "- Your answer shall include two plots presenting compariable measures of the different epsilon settings (e.g. the average reward per step and % of optimal action). **(1 point)** \n",
    "- You shall present the average results from at least 100 simulation runs. Remember that the gambling environment CANNOT be changed over those runs used for calculating the average results. \n",
    "- You may adjust the total time steps when the learning needs more time for a cerain epsilon value, but do not over spend your time on this.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fill in your answer (at most 300 words) with accompanying plots here.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are almost done! Before handing in, make sure that the codes you hand in work, and that all plots are shown. **Submit just one file per team.** Please make sure that you submit a .zip file with images.\n",
    "\n",
    "Again, make your you name this file according to your last names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "num_action = 10\n",
    "num_seed = 42\n",
    "num_runs = 200  # number of simulation runs\n",
    "num_episodes = 500  # number of steps in each run\n",
    "\n",
    "# set up the random seed\n",
    "np.random.seed(num_seed)\n",
    "\n",
    "# init the environment\n",
    "env = GamblingRoom(num_action)\n",
    "\n",
    "# delete the wrap\n",
    "env = env.unwrapped\n",
    "\n",
    "best_action = None\n",
    "best_mu = -100\n",
    "for i, sm in enumerate(env.r_machines):\n",
    "    if best_mu < sm.mu:\n",
    "        best_mu = sm.mu\n",
    "        best_action = i\n",
    "\n",
    "steps_list = []\n",
    "best_action_epsilon = []\n",
    "\n",
    "for i_eps, epsilon in enumerate(np.arange(0, 0.11, 0.05)):\n",
    "    step_ba = np.zeros(num_episodes)\n",
    "    steps_reward = np.zeros((num_runs, num_episodes))\n",
    "    sims = np.zeros(num_runs)\n",
    "    for i_run in range(num_runs):\n",
    "        agent = EplisonGreedyAgent(num_action, epsilon)\n",
    "        reward_tot = 0\n",
    "        for i_step in range(num_episodes):\n",
    "            action = agent.select_action()\n",
    "            reward = env.step(action)[1]\n",
    "            agent.update_parameters(action, reward)\n",
    "            reward_tot += reward\n",
    "            steps_reward[i_run, i_step] = reward\n",
    "            if action == best_action:\n",
    "                step_ba[i_step] += 1\n",
    "        sims[i_run] = reward_tot\n",
    "    steps_avg_reward = np.mean(steps_reward, axis=0)\n",
    "    steps_list.append(steps_avg_reward)\n",
    "    best_action_epsilon.append(step_ba * 100 / num_runs)\n",
    "\n",
    "env.close()\n",
    "\n",
    "# average reward plot\n",
    "plot1, = plt.plot(np.arange(1, 501, 1), np.array(\n",
    "    steps_list[0]), label='eps = 0', linewidth=0.5)\n",
    "\n",
    "plot2, = plt.plot(np.arange(1, 501, 1), np.array(\n",
    "    steps_list[1]), label='eps = 0.05', linewidth=0.5)\n",
    "\n",
    "plot3, = plt.plot(np.arange(1, 501, 1), np.array(\n",
    "    steps_list[2]), label='eps = 0.1', linewidth=0.5)\n",
    "plt.legend(handles=[plot1, plot2, plot3])\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Simulation avg reward\")\n",
    "plt.title(\"Average rewards for each step across multiple simulations\")\n",
    "plt.show()\n",
    "\n",
    "# optimal action plot\n",
    "plot1, = plt.plot(np.arange(1, 501, 1), np.array(\n",
    "    best_action_epsilon[0]), label='eps = 0', linewidth=0.5)\n",
    "\n",
    "plot2, = plt.plot(np.arange(1, 501, 1), np.array(\n",
    "    best_action_epsilon[1]), label='eps = 0.05', linewidth=0.5)\n",
    "\n",
    "plot3, = plt.plot(np.arange(1, 501, 1), np.array(\n",
    "    best_action_epsilon[2]), label='eps = 0.1', linewidth=0.5)\n",
    "plt.legend(handles=[plot1, plot2, plot3])\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Optimal action choice %\")\n",
    "plt.title(\"Optimal action percentage for each step\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "The environment chosen for the experiment is a gambling room with 10 slot machines. Each agent goes through 200 runs, and each run is composed of 500 episodes. After extensive exploration with different environments, it seemed that the best alpha values in terms of average step reward are always in the range between 0 and 0.1. Thus, for this specific environment we chose to experiment with three different alpha values: 0, 0.05 and 0.1. To select the best alpha value among these three, the average step reward and the percentage of optimal choice per step across all simulations were plotted for each alpha value. Results are shown as images in the zip file. \n",
    "As we can see in the average reward plot, at first, the greedy strategy improved a little bit quicker than the other ways, but it subsequently plateaued at a lower level. In contrast to the best feasible reward per step on this testbed of approximately 3.22, it only managed to achieve a reward of roughly 2.57. Because it frequently gets stuck completing ineffective tasks, the greedy technique consistently performs much worse over time. We can also see that the results using an epsilon of 0.05 are slightly higher than using 0.1 as the epsilon value, making it the optimal value in this case.\n",
    "The optimal action graph demonstrates that only about 40% of the jobs were successfully completed using the greedy strategy. It never went back to the optimal action in the remaining section because its initial samples of it were unsatisfying. The ε-greedy approaches eventually outperform the other ways because they keep exploring and increase their odds of finding the best course of action. The epsilon = 0.1 method got a maximum percentage of optimal action choice of 94.5%, 0.5% more than the ε = 0.05 method. In this case, as said earlier, both the non-greedy methods obtained very similar results."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Lesson 1: Multi- Armed Bandit with OpenAi Gym ver 2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('mair')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "e73927171d62f529ca1aa599ee0b84007adc4c716ac382f2cb4878b3f065b61e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
